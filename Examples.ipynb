{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cdbd14-8d51-407e-a3e1-ae7e29bf1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rde import RDE\n",
    "from src.strategies import build_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d0f53-ad02-4c27-9539-5b14e96558e8",
   "metadata": {},
   "source": [
    "## Optimal tracking of fractional Brownian motion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec7b44-1a06-4ce6-bd96-e2ed8d1cf256",
   "metadata": {},
   "source": [
    "### A. Defining the Controlled System\n",
    "\n",
    "We begin by defining the controlled system through a function that specifies the right-hand side of the equation:\n",
    "\n",
    "$$\n",
    "dY_t = b(Y_t, U_t) \\, dt + \\sigma(Y_t) \\, dX_t\n",
    "$$\n",
    "\n",
    "Here, `dX` represents the underlying time-augmented process, structured as follows:\n",
    "\n",
    "```python\n",
    "dX[:, 0] = dt   # Time increment\n",
    "dX[:, 1] = dX_t # Stochastic process increment\n",
    "```\n",
    "In the tracking problem $Y$ will denote the difference between the tracking signal and thetracked process, \n",
    "therefore the system is simply given by\n",
    "$$\n",
    "dY_t = - U_t \\, dt + \\, dX_t\n",
    "$$\n",
    "in code this becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e1ed06-6db9-4c04-9b9a-0c774e13ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rde_model(Y, U, dX):\n",
    "    return dX[:, 1] - U * dX[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19282b0d-ff98-43c7-91bb-cc254d8944f1",
   "metadata": {},
   "source": [
    "### B. Defining the Strategies\n",
    "\n",
    "Next, we define the strategies used to approximate the solutions. Here, we have several degrees of freedom. Specifically, we need to choose between:\n",
    "\n",
    "- **Open-loop** and **closed-loop** strategies\n",
    "- **Linear** and **deep** strategies\n",
    "- **Log-signatures** and **standard signatures**\n",
    "- **Truncation levels**\n",
    "\n",
    "To facilitate these choices, we have implemented the helper method `build_strategy`. \n",
    "\n",
    "Below, we will define several types of strategies and explain their functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da663d3-571f-463e-84b0-57dfe092fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open-loop Linear Strategy\n",
    "lin_strat = build_strategy(\n",
    "    N=3,          # Truncation level\n",
    "    space=\"sig\",  # Standard signature\n",
    "    sig_comp=\"tX\", # Open-loop (time signature of (t, X_t))\n",
    "    nn_hidden=0   # Number of hidden layers; 0 for linear strategies\n",
    ")\n",
    "\n",
    "# Closed-loop DNN Strategy\n",
    "dnn_strat = build_strategy(\n",
    "    N=3,\n",
    "    space=\"log\",  # Log-signature\n",
    "    sig_comp=\"tY\", # Closed-loop (time signature of (t, Y_t))\n",
    "    nn_hidden=2 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fede2bd-facf-4655-8390-e0a738c147ff",
   "metadata": {},
   "source": [
    "The arguments `N`, `space` and `sig_comp` are required. Additionally, a constraint interval can be specified passing `constraint = (a, b)`. For details on the DNN architecture parameters, please refer to the implementation in `src/strategies.py`. \n",
    "Note: A combination of open- and closed-loop strategies is possible by specifying `sig_comp = 'tXY'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f63394-b674-4eea-82bf-7856972016f9",
   "metadata": {},
   "source": [
    "### C. Wrapping-up a trainable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256ca54-77a7-4d60-b223-024d9e7c7a64",
   "metadata": {},
   "source": [
    "We next integrate the RDE model with the strategy models into a single PyTorch model, enabling its use in a training pipeline. This is achieved simply by initializing an instance of the `RDE` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05d4c2f9-cfa7-4e2b-acdd-65e28caa29a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rde_lin = RDE(rde_model=rde_model, **lin_strat)\n",
    "rde_log = RDE(rde_model=rde_model, **dnn_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba87e1-0b1d-47cc-bae0-60aeb880f625",
   "metadata": {},
   "source": [
    "### D. Defining the Loss Function\n",
    "\n",
    "Next we are going to define the loss function of the model, i.e., the minimization objective `L`. In general, this should have the following signature:\n",
    "\n",
    "```python\n",
    "loss_fn(time, Y, U, X, **kwargs)\n",
    "```\n",
    "\n",
    "where `time` is the array of time discretization points, and `Y`, `U`, and `X` are the instance matrices of the corresponding processes. These are represented as arrays of shape `(m, n, d)`, where:\n",
    "- `m` is the number of Monte Carlo samples,\n",
    "- `n` is the number of time steps,\n",
    "- `d` is the dimension of the process.\n",
    "\n",
    "**Note:** Currently, the dimensions of `Y` and `U` must be `1`. Higher-dimensional control functionality will be added soon.\n",
    "\n",
    "For evaluation purposes, the loss function should **not only return the Monte Carlo average of the losses** but also output the sample **variance of the loss**.\n",
    "\n",
    "For the tracking problem, the loss function is given by:\n",
    "\n",
    "$$\n",
    "L(Y, U) = \\frac{1}{2} \\int_0^T \\Big( (Y_t)^2 + \\kappa (U_t)^2 \\Big) d{t},\n",
    "$$\n",
    "\n",
    "where $\\kappa > 0$ is the penalization parameter. We implement this cost functional as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415aa13c-b5e9-4c61-b4ae-3c636427373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PENALTY = 0.1\n",
    "\n",
    "def loss_fn(time, Y, U, X, **kwargs):\n",
    "    payoff = 0.5 * (Y[:, :-1] ** 2 + PENALTY * U[:, :-1] ** 2)\n",
    "    l_ = torch.mean(payoff) * (time[-1] - time[0])\n",
    "    v = torch.var(torch.mean(payoff, dim=1) * (time[-1] - time[0]))\n",
    "    return l_, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b550043-deff-421c-863b-b13f904dfe27",
   "metadata": {},
   "source": [
    "### E. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec9ba899-8abb-4531-873b-8204ea45211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b93fdc-b001-4de5-b0eb-b56fb558073f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17742925-b518-4ccd-9338-0972b1089fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
